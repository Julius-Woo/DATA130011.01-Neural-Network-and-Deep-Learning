{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task4. Add l2 regularization (or l1-regularization) of the weights to your loss function. For neural networks this is\n",
    "called weight decay. An alternate form of regularization that is sometimes used is early stopping, which is\n",
    "stopping training when the error on a validation set stops decreasing.\n",
    "\n",
    "To check whether the model is overfitting, we print both the training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "def standardizeCols(M, mu=None, sigma2=None):\n",
    "    M = M.astype(float)  # Ensure M is float for precision\n",
    "    nrows, ncols = M.shape\n",
    "    if mu is None or sigma2 is None:\n",
    "        mu = np.mean(M, axis=0)\n",
    "        sigma2 = np.std(M, axis=0)\n",
    "        sigma2[sigma2 < np.finfo(float).eps] = 1  # Avoid division by zero\n",
    "    S = M - mu  # Subtract mean\n",
    "    if ncols > 0:\n",
    "        S = S / sigma2  # Divide by standard deviation\n",
    "    return S, mu, sigma2\n",
    "\n",
    "\n",
    "def form_weights(w, nVars, nHidden, nLabels):\n",
    "    offset = 0\n",
    "    inputWeights = w[offset:nVars * nHidden[0]].reshape(nVars, nHidden[0])\n",
    "    offset += nVars * nHidden[0]\n",
    "    hiddenWeights = []\n",
    "    for h in range(1, len(nHidden)):\n",
    "        size = nHidden[h-1] * nHidden[h]\n",
    "        hiddenWeights.append(\n",
    "            w[offset:offset+size].reshape(nHidden[h-1], nHidden[h]))\n",
    "        offset += size\n",
    "    outputWeights = w[offset:offset + nHidden[-1]\n",
    "                      * nLabels].reshape(nHidden[-1], nLabels)\n",
    "    return inputWeights, hiddenWeights, outputWeights\n",
    "\n",
    "\n",
    "def sech2(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPclassificationPredict(w, X, nHidden, nLabels):\n",
    "    nInstances, nVars = X.shape\n",
    "    inputWeights, hiddenWeights, outputWeights = form_weights(w, nVars, nHidden, nLabels)\n",
    "    activations = X\n",
    "    for h in range(len(nHidden)):\n",
    "        activations = np.tanh(activations @ (inputWeights if h == 0 else hiddenWeights[h-1]))\n",
    "    y = activations @ outputWeights\n",
    "    y = np.argmax(y, axis=1, keepdims=True) + 1\n",
    "    return y\n",
    "\n",
    "def MLPclassificationLoss(w, X, y, nHidden, nLabels):\n",
    "    nInstances, nVars = X.shape\n",
    "    inputWeights, hiddenWeights, outputWeights = form_weights(w, nVars, nHidden, nLabels)\n",
    "\n",
    "    activations = [X]\n",
    "    for h in range(len(nHidden)):\n",
    "        z = activations[-1] @ (inputWeights if h == 0 else hiddenWeights[h-1])\n",
    "        a = np.tanh(z)\n",
    "        activations.append(a)\n",
    "    yhat = activations[-1] @ outputWeights\n",
    "    f = np.sum((yhat - y)**2)  # Loss\n",
    "\n",
    "    gOutput = 2 * activations[-1].T @ (yhat - y)\n",
    "    gHidden = []\n",
    "    delta = 2 * (yhat - y) @ outputWeights.T * sech2(activations[-1])\n",
    "    for h in range(len(nHidden) - 1, 0, -1):\n",
    "        gHidden.append(activations[h].T @ delta)\n",
    "        delta = (delta @ hiddenWeights[h-1].T) * sech2(activations[h])\n",
    "    gHidden.append(activations[0].T @ delta)\n",
    "    gHidden.reverse()\n",
    "\n",
    "    gradients = [gHidden[0].flatten()]\n",
    "    for g in gHidden[1:]:\n",
    "        gradients.append(g.flatten())\n",
    "    gradients.append(gOutput.flatten())\n",
    "    g = np.concatenate(gradients)\n",
    "\n",
    "    return f, g.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration = 10000, training error = 0.295600, validation error = 0.315600\n",
      "Training iteration = 20000, training error = 0.290000, validation error = 0.313800\n",
      "Training iteration = 30000, training error = 0.186800, validation error = 0.197600\n",
      "Training iteration = 40000, training error = 0.206000, validation error = 0.227600\n",
      "Training iteration = 50000, training error = 0.187600, validation error = 0.214600\n",
      "Training iteration = 60000, training error = 0.174800, validation error = 0.200200\n",
      "Training iteration = 70000, training error = 0.176800, validation error = 0.202600\n",
      "Training iteration = 80000, training error = 0.199200, validation error = 0.221800\n",
      "Early stopping triggered after 80000 iterations with validation error 0.197600\n",
      "Test error with final model = 0.211000\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('digits.mat')\n",
    "X = data['X']\n",
    "y = data['y'].flatten()\n",
    "yvalid = data['yvalid']\n",
    "ytest = data['ytest']\n",
    "\n",
    "n, d = X.shape  # 5000, 256\n",
    "nLabels = np.max(y)  # 10\n",
    "yExpanded = 2 * np.eye(nLabels)[y - 1] - 1  # turn into one-hot vector\n",
    "t = data['Xvalid'].shape[0]  # 5000\n",
    "t2 = data['Xtest'].shape[0]  # 1000\n",
    "\n",
    "\n",
    "# Standardize columns and add bias\n",
    "X, mu, sigma = standardizeCols(X)\n",
    "X = np.hstack([np.ones((n, 1)), X])\n",
    "\n",
    "# Apply the same transformation to the validation/test data\n",
    "Xvalid, _, _ = standardizeCols(data['Xvalid'], mu, sigma)\n",
    "Xvalid = np.hstack([np.ones((t, 1)), Xvalid])\n",
    "\n",
    "Xtest, _, _ = standardizeCols(data['Xtest'], mu, sigma)\n",
    "Xtest = np.hstack([np.ones((t2, 1)), Xtest])\n",
    "\n",
    "\n",
    "nHidden = [10]\n",
    "d += 1\n",
    "\n",
    "nParams = d * nHidden[0]\n",
    "nParams += sum(nHidden[h-1] * nHidden[h] for h in range(1, len(nHidden)))\n",
    "nParams += nHidden[-1] * nLabels\n",
    "maxIter = 100000\n",
    "initialStepSize = 1e-3\n",
    "stepSize = initialStepSize\n",
    "decayRate = 1e-5\n",
    "momentum = 0.9\n",
    "\n",
    "w = np.random.randn(nParams, 1)\n",
    "w_diff = np.zeros_like(w)\n",
    "best_w = np.copy(w)\n",
    "best_validation_error = float('inf')\n",
    "patience = 5  # Number of iterations to wait before early stopping\n",
    "patience_counter = 0\n",
    "\n",
    "\n",
    "for iter in range(0, maxIter):\n",
    "    stepSize = initialStepSize * (1 / (1 + decayRate * iter))\n",
    "    if (iter + 1) % (maxIter // 10) == 0:\n",
    "        yhat1 = MLPclassificationPredict(w, X, nHidden, nLabels)\n",
    "        train_error = np.mean(yhat1 != y.reshape(-1, 1))\n",
    "        yhat = MLPclassificationPredict(w, Xvalid, nHidden, nLabels)\n",
    "        validation_error = np.mean(yhat != yvalid)\n",
    "        print(f'Training iteration = {iter + 1}, training error = {train_error:.6f}, validation error = {validation_error:.6f}')\n",
    "            \n",
    "        # Early Stopping check\n",
    "        if validation_error < best_validation_error:\n",
    "            best_validation_error = validation_error\n",
    "            best_w = np.copy(w)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {iter + 1} iterations with validation error {best_validation_error:.6f}\")\n",
    "                break\n",
    "    i = np.random.randint(n)\n",
    "    f, g = MLPclassificationLoss(\n",
    "        w, X[i:i+1], yExpanded[i:i+1], nHidden, nLabels)\n",
    "    w_diff = momentum * w_diff - stepSize * g\n",
    "    w += w_diff\n",
    "\n",
    "# Evaluate test error\n",
    "yhat = MLPclassificationPredict(best_w, Xtest, nHidden, nLabels)\n",
    "test_error = np.mean(yhat != ytest)\n",
    "print(f'Test error with final model = {test_error:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that the code actually works, we set the `patience` parameter to a small value. And indeed this gets better test accuracy than without early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPclassificationLoss_l2(w, X, y, nHidden, nLabels, reg_lambda):\n",
    "    nInstances, nVars = X.shape\n",
    "    inputWeights, hiddenWeights, outputWeights = form_weights(\n",
    "        w, nVars, nHidden, nLabels)\n",
    "\n",
    "    activations = [X]\n",
    "    for h in range(len(nHidden)):\n",
    "        z = activations[-1] @ (inputWeights if h == 0 else hiddenWeights[h-1])\n",
    "        a = np.tanh(z)\n",
    "        activations.append(a)\n",
    "    yhat = activations[-1] @ outputWeights\n",
    "\n",
    "    # Add L2 regularization term to the loss function\n",
    "    reg_loss = (reg_lambda / 2) * np.sum(w**2)\n",
    "    total_loss = np.sum((yhat - y)**2) + reg_loss\n",
    "\n",
    "    gOutput = 2 * activations[-1].T @ (yhat - y) + reg_lambda * outputWeights\n",
    "    gHidden = []\n",
    "    delta = 2 * (yhat - y) @ outputWeights.T * sech2(activations[-1])\n",
    "    for h in range(len(nHidden) - 1, 0, -1):\n",
    "        gHidden.append(activations[h].T @ delta +\n",
    "                       reg_lambda * hiddenWeights[h-1])\n",
    "        delta = (delta @ hiddenWeights[h-1].T) * sech2(activations[h])\n",
    "    gHidden.append(activations[0].T @ delta + reg_lambda * inputWeights)\n",
    "    gHidden.reverse()\n",
    "\n",
    "    gradients = [gHidden[0].flatten()]\n",
    "    for g in gHidden[1:]:\n",
    "        gradients.append(g.flatten())\n",
    "    gradients.append(gOutput.flatten())\n",
    "    g = np.concatenate(gradients)\n",
    "\n",
    "    return total_loss, g.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Training with regularization lambda = 1.0-----\n",
      "Training iteration = 10000, training error = 0.588800, validation error = 0.596200\n",
      "Training iteration = 20000, training error = 0.503400, validation error = 0.524800\n",
      "Training iteration = 30000, training error = 0.463800, validation error = 0.461600\n",
      "Training iteration = 40000, training error = 0.529200, validation error = 0.540400\n",
      "Training iteration = 50000, training error = 0.365400, validation error = 0.370000\n",
      "Training iteration = 60000, training error = 0.480400, validation error = 0.510400\n",
      "Training iteration = 70000, training error = 0.493200, validation error = 0.527600\n",
      "Training iteration = 80000, training error = 0.455400, validation error = 0.463600\n",
      "Training iteration = 90000, training error = 0.376400, validation error = 0.377000\n",
      "Training iteration = 100000, training error = 0.388000, validation error = 0.393800\n",
      "Test error with final model = 0.386000\n",
      "-----Training with regularization lambda = 0.5-----\n",
      "Training iteration = 10000, training error = 0.343400, validation error = 0.336200\n",
      "Training iteration = 20000, training error = 0.339400, validation error = 0.357800\n",
      "Training iteration = 30000, training error = 0.385400, validation error = 0.376200\n",
      "Training iteration = 40000, training error = 0.322000, validation error = 0.356200\n",
      "Training iteration = 50000, training error = 0.268600, validation error = 0.285200\n",
      "Training iteration = 60000, training error = 0.235000, validation error = 0.233000\n",
      "Training iteration = 70000, training error = 0.234600, validation error = 0.246400\n",
      "Training iteration = 80000, training error = 0.279800, validation error = 0.289600\n",
      "Training iteration = 90000, training error = 0.200600, validation error = 0.212800\n",
      "Training iteration = 100000, training error = 0.200600, validation error = 0.206400\n",
      "Test error with final model = 0.210000\n",
      "-----Training with regularization lambda = 0.2-----\n",
      "Training iteration = 10000, training error = 0.226000, validation error = 0.241000\n",
      "Training iteration = 20000, training error = 0.227200, validation error = 0.244200\n",
      "Training iteration = 30000, training error = 0.194000, validation error = 0.206600\n",
      "Training iteration = 40000, training error = 0.188000, validation error = 0.196800\n",
      "Training iteration = 50000, training error = 0.209800, validation error = 0.221000\n",
      "Training iteration = 60000, training error = 0.217600, validation error = 0.228600\n",
      "Training iteration = 70000, training error = 0.191600, validation error = 0.195400\n",
      "Training iteration = 80000, training error = 0.203600, validation error = 0.216000\n",
      "Training iteration = 90000, training error = 0.185000, validation error = 0.192800\n",
      "Training iteration = 100000, training error = 0.196200, validation error = 0.216400\n",
      "Test error with final model = 0.194000\n",
      "-----Training with regularization lambda = 0.1-----\n",
      "Training iteration = 10000, training error = 0.212600, validation error = 0.237200\n",
      "Training iteration = 20000, training error = 0.189600, validation error = 0.207800\n",
      "Training iteration = 30000, training error = 0.161000, validation error = 0.181800\n",
      "Training iteration = 40000, training error = 0.166000, validation error = 0.178000\n",
      "Training iteration = 50000, training error = 0.160200, validation error = 0.182800\n",
      "Training iteration = 60000, training error = 0.175400, validation error = 0.197800\n",
      "Training iteration = 70000, training error = 0.150800, validation error = 0.169600\n",
      "Training iteration = 80000, training error = 0.172400, validation error = 0.179400\n",
      "Training iteration = 90000, training error = 0.158400, validation error = 0.177600\n",
      "Training iteration = 100000, training error = 0.160200, validation error = 0.172200\n",
      "Test error with final model = 0.165000\n",
      "-----Training with regularization lambda = 0.01-----\n",
      "Training iteration = 10000, training error = 0.126800, validation error = 0.154400\n",
      "Training iteration = 20000, training error = 0.132600, validation error = 0.164400\n",
      "Training iteration = 30000, training error = 0.128400, validation error = 0.149600\n",
      "Training iteration = 40000, training error = 0.131000, validation error = 0.156600\n",
      "Training iteration = 50000, training error = 0.114400, validation error = 0.143800\n",
      "Training iteration = 60000, training error = 0.135800, validation error = 0.167000\n",
      "Training iteration = 70000, training error = 0.108800, validation error = 0.136400\n",
      "Training iteration = 80000, training error = 0.113800, validation error = 0.145000\n",
      "Training iteration = 90000, training error = 0.112200, validation error = 0.127000\n",
      "Training iteration = 100000, training error = 0.137800, validation error = 0.165800\n",
      "Test error with final model = 0.142000\n",
      "-----Training with regularization lambda = 0.001-----\n",
      "Training iteration = 10000, training error = 0.127600, validation error = 0.155200\n",
      "Training iteration = 20000, training error = 0.143800, validation error = 0.179200\n",
      "Training iteration = 30000, training error = 0.155000, validation error = 0.181600\n",
      "Training iteration = 40000, training error = 0.125400, validation error = 0.153000\n",
      "Training iteration = 50000, training error = 0.142400, validation error = 0.171800\n",
      "Training iteration = 60000, training error = 0.154600, validation error = 0.179200\n",
      "Training iteration = 70000, training error = 0.163000, validation error = 0.186800\n",
      "Training iteration = 80000, training error = 0.160200, validation error = 0.187600\n",
      "Training iteration = 90000, training error = 0.176200, validation error = 0.194200\n",
      "Training iteration = 100000, training error = 0.171400, validation error = 0.193800\n",
      "Test error with final model = 0.142000\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('digits.mat')\n",
    "X = data['X']\n",
    "y = data['y'].flatten()\n",
    "yvalid = data['yvalid']\n",
    "ytest = data['ytest']\n",
    "n, d = X.shape  # 5000, 256\n",
    "nLabels = np.max(y)  # 10\n",
    "yExpanded = 2 * np.eye(nLabels)[y - 1] - 1  # turn into one-hot vector\n",
    "t = data['Xvalid'].shape[0]  # 5000\n",
    "t2 = data['Xtest'].shape[0]  # 1000\n",
    "X, mu, sigma = standardizeCols(X)\n",
    "X = np.hstack([np.ones((n, 1)), X])\n",
    "Xvalid, _, _ = standardizeCols(data['Xvalid'], mu, sigma)\n",
    "Xvalid = np.hstack([np.ones((t, 1)), Xvalid])\n",
    "Xtest, _, _ = standardizeCols(data['Xtest'], mu, sigma)\n",
    "Xtest = np.hstack([np.ones((t2, 1)), Xtest])\n",
    "\n",
    "nHidden = [10]\n",
    "d += 1\n",
    "\n",
    "nParams = d * nHidden[0]\n",
    "nParams += sum(nHidden[h-1] * nHidden[h] for h in range(1, len(nHidden)))\n",
    "nParams += nHidden[-1] * nLabels\n",
    "maxIter = 100000\n",
    "initialStepSize = 1e-3\n",
    "decayRate = 1e-5\n",
    "momentum = 0.9\n",
    "reg_lambdas = [1.0, 0.5, 0.2, 0.1, 0.01, 0.001]\n",
    "\n",
    "w = np.random.randn(nParams, 1)\n",
    "w_diff = np.zeros_like(w)\n",
    "best_w = np.copy(w)\n",
    "best_validation_error = float('inf')\n",
    "\n",
    "for reg_lambda in reg_lambdas:\n",
    "    print(f\"-----Training with regularization lambda = {reg_lambda}-----\")\n",
    "    for iter in range(0, maxIter):\n",
    "        stepSize = initialStepSize * (1 / (1 + decayRate * iter))\n",
    "        if (iter + 1) % (maxIter // 10) == 0:\n",
    "            yhat1 = MLPclassificationPredict(w, X, nHidden, nLabels)\n",
    "            train_error = np.mean(yhat1 != y.reshape(-1, 1))\n",
    "            yhat = MLPclassificationPredict(w, Xvalid, nHidden, nLabels)\n",
    "            validation_error = np.mean(yhat != yvalid)\n",
    "            print(f'Training iteration = {iter + 1}, training error = {train_error:.6f}, validation error = {validation_error:.6f}')\n",
    "\n",
    "            if validation_error < best_validation_error:\n",
    "                best_validation_error = validation_error\n",
    "                best_w = np.copy(w)\n",
    "        \n",
    "        i = np.random.randint(n)\n",
    "        _, g = MLPclassificationLoss_l2(\n",
    "            w, X[i:i+1], yExpanded[i:i+1], nHidden, nLabels, reg_lambda)\n",
    "        w_diff = momentum * w_diff - stepSize * g\n",
    "        w += w_diff\n",
    "\n",
    "    # Evaluate test error\n",
    "    yhat = MLPclassificationPredict(best_w, Xtest, nHidden, nLabels)\n",
    "    test_error = np.mean(yhat != ytest)\n",
    "    print(f'Test error with final model = {test_error:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a small value of `l2` is preferred, as a large value will make the model underfit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
