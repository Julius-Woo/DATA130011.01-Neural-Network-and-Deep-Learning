{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task3. Vectorize evaluating the loss function (e.g., try to express as much as possible in terms of matrix\n",
    "operations), to do more training iterations in a reasonable amount of time.\n",
    "\n",
    "We have already improved the loss function in task1, and it significantly speeds up the training process. Here we compare the training time of the vectorized and original loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "def standardizeCols(M, mu=None, sigma2=None):\n",
    "    M = M.astype(float)  # Ensure M is float for precision\n",
    "    nrows, ncols = M.shape\n",
    "    if mu is None or sigma2 is None:\n",
    "        mu = np.mean(M, axis=0)\n",
    "        sigma2 = np.std(M, axis=0)\n",
    "        sigma2[sigma2 < np.finfo(float).eps] = 1  # Avoid division by zero\n",
    "    S = M - mu  # Subtract mean\n",
    "    if ncols > 0:\n",
    "        S = S / sigma2  # Divide by standard deviation\n",
    "    return S, mu, sigma2\n",
    "\n",
    "\n",
    "def form_weights(w, nVars, nHidden, nLabels):\n",
    "    offset = 0\n",
    "    inputWeights = w[offset:nVars * nHidden[0]].reshape(nVars, nHidden[0])\n",
    "    offset += nVars * nHidden[0]\n",
    "    hiddenWeights = []\n",
    "    for h in range(1, len(nHidden)):\n",
    "        size = nHidden[h-1] * nHidden[h]\n",
    "        hiddenWeights.append(\n",
    "            w[offset:offset+size].reshape(nHidden[h-1], nHidden[h]))\n",
    "        offset += size\n",
    "    outputWeights = w[offset:offset + nHidden[-1]\n",
    "                      * nLabels].reshape(nHidden[-1], nLabels)\n",
    "    return inputWeights, hiddenWeights, outputWeights\n",
    "\n",
    "\n",
    "def sech2(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original version\n",
    "def MLPclassificationPredict(w, X, nHidden, nLabels):\n",
    "    nInstances, nVars = X.shape\n",
    "\n",
    "    # Form Weights\n",
    "    inputWeights, hiddenWeights, outputWeights = form_weights(\n",
    "        w, nVars, nHidden, nLabels)\n",
    "\n",
    "    # Compute Output\n",
    "    y = np.zeros((nInstances, nLabels))\n",
    "    for i in range(nInstances):\n",
    "        ip = [X[i] @ inputWeights]\n",
    "        fp = [np.tanh(ip[0])]\n",
    "        for h in range(1, len(nHidden)):\n",
    "            ip.append(fp[h-1] @ hiddenWeights[h-1])\n",
    "            fp.append(np.tanh(ip[h]))\n",
    "        y[i] = fp[-1] @ outputWeights\n",
    "\n",
    "    # Pick the class with the highest score\n",
    "    y = np.argmax(y, axis=1, keepdims=True) + 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def MLPclassificationLoss(w, X, y, nHidden, nLabels):\n",
    "    # X, y are all 2-D arrays\n",
    "    nInstances, nVars = X.shape\n",
    "    inputWeights, hiddenWeights, outputWeights = form_weights(\n",
    "        w, nVars, nHidden, nLabels)\n",
    "\n",
    "    f = 0  # Loss\n",
    "    gInput = np.zeros_like(inputWeights)  # Gradient for input weights\n",
    "    # Gradient for hidden weights\n",
    "    gHidden = [np.zeros_like(hw) for hw in hiddenWeights]\n",
    "    gOutput = np.zeros_like(outputWeights)  # Gradient for output weights\n",
    "\n",
    "    # Compute Output\n",
    "    for i in range(nInstances):\n",
    "        ip = [None] * len(nHidden)\n",
    "        fp = [None] * len(nHidden)\n",
    "\n",
    "        ip[0] = X[i, :] @ inputWeights  # 1-D array\n",
    "        fp[0] = np.tanh(ip[0])  # 1-D array\n",
    "        for h in range(1, len(nHidden)):\n",
    "            ip[h] = fp[h-1] @ hiddenWeights[h-1]\n",
    "            fp[h] = np.tanh(ip[h])\n",
    "        yhat = fp[-1] @ outputWeights\n",
    "\n",
    "        relativeErr = yhat - y[i, :]\n",
    "        f += np.sum(relativeErr**2)\n",
    "\n",
    "        err = 2 * relativeErr\n",
    "\n",
    "        # Output Weights Gradient\n",
    "        gOutput += np.outer(fp[-1], err)\n",
    "\n",
    "        if len(nHidden) > 1:\n",
    "            backprop = np.zeros_like(fp[-1])\n",
    "            for c in range(nLabels):\n",
    "                backprop[c] = err[c] * sech2(ip[-1]) * outputWeights[:, c].T\n",
    "                gHidden[-1] += np.outer(fp[-2], backprop[c])\n",
    "            backprop = np.sum(backprop, axis=0)  # Sum over classes\n",
    "\n",
    "            # Other Hidden Layer Gradients\n",
    "            for h in range(len(nHidden)-2, -1, -1):\n",
    "                backprop = (backprop @ hiddenWeights[h].T) * sech2(ip[h])\n",
    "                if h > 0:\n",
    "                    gHidden[h-1] += np.outer(fp[h-1], backprop)\n",
    "                else:\n",
    "                    # Input weights\n",
    "                    gInput += np.outer(X[i, :], backprop)\n",
    "        else:\n",
    "            # Input Weights Gradient for single hidden layer case\n",
    "            for c in range(nLabels):\n",
    "                gInput += err[c] * \\\n",
    "                    np.outer(X[i], sech2(ip[-1]) * outputWeights[:, c].T)\n",
    "\n",
    "    # Put Gradient into vector\n",
    "    g = np.zeros_like(w)\n",
    "    g[:nVars * nHidden[0]] = gInput.reshape(-1, 1)\n",
    "    offset = nVars * nHidden[0]\n",
    "    for h in range(len(nHidden)-1):\n",
    "        g[offset: offset + nHidden[h] * nHidden[h+1]\n",
    "          ] = gHidden[h].reshape(-1, 1)\n",
    "        offset += nHidden[h] * nHidden[h+1]\n",
    "    g[offset: offset + nHidden[-1] * nLabels] = gOutput.reshape(-1, 1)\n",
    "\n",
    "    return f, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster version\n",
    "def MLPclassificationPredict_fast(w, X, nHidden, nLabels):\n",
    "    nInstances, nVars = X.shape\n",
    "    inputWeights, hiddenWeights, outputWeights = form_weights(w, nVars, nHidden, nLabels)\n",
    "    activations = X\n",
    "    for h in range(len(nHidden)):\n",
    "        activations = np.tanh(activations @ (inputWeights if h == 0 else hiddenWeights[h-1]))\n",
    "    y = activations @ outputWeights\n",
    "    y = np.argmax(y, axis=1, keepdims=True) + 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def MLPclassificationLoss_fast(w, X, y, nHidden, nLabels):\n",
    "    nInstances, nVars = X.shape\n",
    "    inputWeights, hiddenWeights, outputWeights = form_weights(w, nVars, nHidden, nLabels)\n",
    "\n",
    "    activations = [X]\n",
    "    for h in range(len(nHidden)):\n",
    "        z = activations[-1] @ (inputWeights if h == 0 else hiddenWeights[h-1])\n",
    "        a = np.tanh(z)\n",
    "        activations.append(a)\n",
    "    yhat = activations[-1] @ outputWeights\n",
    "    f = np.sum((yhat - y)**2)  # Loss\n",
    "\n",
    "    gOutput = 2 * activations[-1].T @ (yhat - y)\n",
    "    gHidden = []\n",
    "    delta = 2 * (yhat - y) @ outputWeights.T * sech2(activations[-1])\n",
    "    for h in range(len(nHidden) - 1, 0, -1):\n",
    "        gHidden.append(activations[h].T @ delta)\n",
    "        delta = (delta @ hiddenWeights[h-1].T) * sech2(activations[h])\n",
    "    gHidden.append(activations[0].T @ delta)\n",
    "    gHidden.reverse()\n",
    "\n",
    "    gradients = [gHidden[0].flatten()]\n",
    "    for g in gHidden[1:]:\n",
    "        gradients.append(g.flatten())\n",
    "    gradients.append(gOutput.flatten())\n",
    "    g = np.concatenate(gradients)\n",
    "\n",
    "    return f, g.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('digits.mat')\n",
    "X = data['X']\n",
    "y = data['y'].flatten()\n",
    "yvalid = data['yvalid']\n",
    "ytest = data['ytest']\n",
    "\n",
    "n, d = X.shape  # 5000, 256\n",
    "nLabels = np.max(y)  # 10\n",
    "yExpanded = 2 * np.eye(nLabels)[y - 1] - 1  # turn into one-hot vector\n",
    "t = data['Xvalid'].shape[0]  # 5000\n",
    "t2 = data['Xtest'].shape[0]  # 1000\n",
    "\n",
    "\n",
    "# Standardize columns and add bias\n",
    "X, mu, sigma = standardizeCols(X)\n",
    "X = np.hstack([np.ones((n, 1)), X])\n",
    "\n",
    "# Apply the same transformation to the validation/test data\n",
    "Xvalid, _, _ = standardizeCols(data['Xvalid'], mu, sigma)\n",
    "Xvalid = np.hstack([np.ones((t, 1)), Xvalid])\n",
    "\n",
    "Xtest, _, _ = standardizeCols(data['Xtest'], mu, sigma)\n",
    "Xtest = np.hstack([np.ones((t2, 1)), Xtest])\n",
    "\n",
    "\n",
    "nHidden = [10]\n",
    "d += 1\n",
    "\n",
    "nParams = d * nHidden[0]\n",
    "nParams += sum(nHidden[h-1] * nHidden[h] for h in range(1, len(nHidden)))\n",
    "nParams += nHidden[-1] * nLabels\n",
    "maxIter = 100000\n",
    "initialStepSize = 1e-3\n",
    "decayRate = 1e-5\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration = 10000, validation error = 0.553800\n",
      "Training iteration = 20000, validation error = 0.408000\n",
      "Training iteration = 30000, validation error = 0.415800\n",
      "Training iteration = 40000, validation error = 0.330000\n",
      "Training iteration = 50000, validation error = 0.421800\n",
      "Training iteration = 60000, validation error = 0.306000\n",
      "Training iteration = 70000, validation error = 0.309600\n",
      "Training iteration = 80000, validation error = 0.301800\n",
      "Training iteration = 90000, validation error = 0.287600\n",
      "Training iteration = 100000, validation error = 0.259800\n",
      "Test error with final model = 0.248000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "time_slow = time.time()\n",
    "w = np.random.randn(nParams, 1)\n",
    "w_diff = np.zeros_like(w)\n",
    "for iter in range(0, maxIter):\n",
    "    stepSize = initialStepSize * (1 / (1 + decayRate * iter))\n",
    "    if (iter + 1) % (maxIter // 10) == 0:\n",
    "        yhat = MLPclassificationPredict(w, Xvalid, nHidden, nLabels)\n",
    "        validation_error = np.mean(yhat != yvalid)\n",
    "        print(f'Training iteration = {iter + 1}, validation error = {validation_error:.6f}')\n",
    "    i = np.random.randint(n)\n",
    "    f, g = MLPclassificationLoss(\n",
    "        w, X[i:i+1], yExpanded[i:i+1], nHidden, nLabels)\n",
    "    w_diff = momentum * w_diff - stepSize * g\n",
    "    w += w_diff\n",
    "\n",
    "# Evaluate test error\n",
    "yhat = MLPclassificationPredict(w, Xtest, nHidden, nLabels)\n",
    "test_error = np.mean(yhat != ytest)\n",
    "print(f'Test error with final model = {test_error:.6f}')\n",
    "time_slow = time.time() - time_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration = 10000, validation error = 0.266000\n",
      "Training iteration = 20000, validation error = 0.213400\n",
      "Training iteration = 30000, validation error = 0.219800\n",
      "Training iteration = 40000, validation error = 0.205600\n",
      "Training iteration = 50000, validation error = 0.242800\n",
      "Training iteration = 60000, validation error = 0.208200\n",
      "Training iteration = 70000, validation error = 0.213000\n",
      "Training iteration = 80000, validation error = 0.230800\n",
      "Training iteration = 90000, validation error = 0.201400\n",
      "Training iteration = 100000, validation error = 0.213200\n",
      "Test error with final model = 0.220000\n",
      "Time taken for slow version: 74.60s\n",
      "Time taken for fast version: 10.43s\n",
      "Fast version is 7.15 times faster\n"
     ]
    }
   ],
   "source": [
    "time_fast = time.time()\n",
    "w = np.random.randn(nParams, 1)\n",
    "w_diff = np.zeros_like(w)\n",
    "for iter in range(0, maxIter):\n",
    "    stepSize = initialStepSize * (1 / (1 + decayRate * iter))\n",
    "    if (iter + 1) % (maxIter // 10) == 0:\n",
    "        yhat = MLPclassificationPredict_fast(w, Xvalid, nHidden, nLabels)\n",
    "        validation_error = np.mean(yhat != yvalid)\n",
    "        print(f'Training iteration = {iter + 1}, validation error = {validation_error:.6f}')\n",
    "    i = np.random.randint(n)\n",
    "    f, g = MLPclassificationLoss_fast(\n",
    "        w, X[i:i+1], yExpanded[i:i+1], nHidden, nLabels)\n",
    "    w_diff = momentum * w_diff - stepSize * g\n",
    "    w += w_diff\n",
    "\n",
    "# Evaluate test error\n",
    "yhat = MLPclassificationPredict_fast(w, Xtest, nHidden, nLabels)\n",
    "test_error = np.mean(yhat != ytest)\n",
    "print(f'Test error with final model = {test_error:.6f}')\n",
    "time_fast = time.time() - time_fast\n",
    "\n",
    "print(f'Time taken for slow version: {time_slow:.2f}s')\n",
    "print(f'Time taken for fast version: {time_fast:.2f}s')\n",
    "print(f'Fast version is {time_slow / time_fast:.2f} times faster')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
