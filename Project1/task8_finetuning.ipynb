{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task8. Do ‘fine-tuning’ of the last layer. Fix the parameters of all the layers except the last one, and solve for the parameters of the last layer exactly as a convex optimization problem. E.g., treat the input to the last layer as the features and use techniques from earlier in the course (this is particularly fast if you use the squared error, since it has a closed-form solution).\n",
    "\n",
    "To gain a closed-form solution, we can use the squared error loss function, as used in Tasks 1-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "def standardizeCols(M, mu=None, sigma2=None):\n",
    "    M = M.astype(float)  # Ensure M is float for precision\n",
    "    nrows, ncols = M.shape\n",
    "    if mu is None or sigma2 is None:\n",
    "        mu = np.mean(M, axis=0)\n",
    "        sigma2 = np.std(M, axis=0)\n",
    "        sigma2[sigma2 < np.finfo(float).eps] = 1  # Avoid division by zero\n",
    "    S = M - mu  # Subtract mean\n",
    "    if ncols > 0:\n",
    "        S = S / sigma2  # Divide by standard deviation\n",
    "    return S, mu, sigma2\n",
    "\n",
    "\n",
    "def form_weights(w, nVars, nHidden, nLabels):\n",
    "    offset = 0\n",
    "    inputWeights = w[offset:nVars * nHidden[0]].reshape(nVars, nHidden[0])\n",
    "    offset += nVars * nHidden[0]\n",
    "    hiddenWeights = []\n",
    "    for h in range(1, len(nHidden)):\n",
    "        size = nHidden[h-1] * nHidden[h]\n",
    "        hiddenWeights.append(\n",
    "            w[offset:offset+size].reshape(nHidden[h-1], nHidden[h]))\n",
    "        offset += size\n",
    "    outputWeights = w[offset:offset + nHidden[-1]\n",
    "                      * nLabels].reshape(nHidden[-1], nLabels)\n",
    "    return inputWeights, hiddenWeights, outputWeights\n",
    "\n",
    "\n",
    "def sech2(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPclassificationPredict(w, X, nHidden, nLabels):\n",
    "    nInstances, nVars = X.shape\n",
    "    inputWeights, hiddenWeights, outputWeights = form_weights(\n",
    "        w, nVars, nHidden, nLabels)\n",
    "    activations = X\n",
    "    for h in range(len(nHidden)):\n",
    "        activations = np.tanh(\n",
    "            activations @ (inputWeights if h == 0 else hiddenWeights[h-1]))\n",
    "    y = activations @ outputWeights\n",
    "    y = np.argmax(y, axis=1, keepdims=True) + 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def MLPclassificationLoss(w, X, y, nHidden, nLabels, reg_lambda):\n",
    "    nInstances, nVars = X.shape\n",
    "    inputWeights, hiddenWeights, outputWeights = form_weights(\n",
    "        w, nVars, nHidden, nLabels)\n",
    "\n",
    "    activations = [X]\n",
    "    for h in range(len(nHidden)):\n",
    "        z = activations[-1] @ (inputWeights if h == 0 else hiddenWeights[h-1])\n",
    "        a = np.tanh(z)\n",
    "        activations.append(a)\n",
    "    yhat = activations[-1] @ outputWeights\n",
    "\n",
    "    # Add L2 regularization term to the loss function\n",
    "    reg_loss = (reg_lambda / 2) * np.sum(w**2)\n",
    "    total_loss = 1/2 * np.sum((yhat - y)**2) + reg_loss\n",
    "\n",
    "    gOutput = activations[-1].T @ (yhat - y) + reg_lambda * outputWeights\n",
    "    gHidden = []\n",
    "    delta = (yhat - y) @ outputWeights.T * sech2(activations[-1])\n",
    "    for h in range(len(nHidden) - 1, 0, -1):\n",
    "        gHidden.append(activations[h].T @ delta +\n",
    "                       reg_lambda * hiddenWeights[h-1])\n",
    "        delta = (delta @ hiddenWeights[h-1].T) * sech2(activations[h])\n",
    "    gHidden.append(activations[0].T @ delta + reg_lambda * inputWeights)\n",
    "    gHidden.reverse()\n",
    "\n",
    "    gradients = [gHidden[0].flatten()]\n",
    "    for g in gHidden[1:]:\n",
    "        gradients.append(g.flatten())\n",
    "    gradients.append(gOutput.flatten())\n",
    "    g = np.concatenate(gradients)\n",
    "\n",
    "    return total_loss, g.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuned_w(w, X, y, nHidden, nLabels, reg_lambda):\n",
    "    nInstances, nVars = X.shape\n",
    "    offset = 0\n",
    "    inputWeights = w[offset:nVars * nHidden[0]].reshape(nVars, nHidden[0])\n",
    "    offset += nVars * nHidden[0]\n",
    "    hiddenWeights = []\n",
    "    for h in range(1, len(nHidden)):\n",
    "        size = nHidden[h-1] * nHidden[h]\n",
    "        hiddenWeights.append(\n",
    "            w[offset:offset+size].reshape(nHidden[h-1], nHidden[h]))\n",
    "        offset += size\n",
    "    outputWeights = w[offset:offset + nHidden[-1]\n",
    "                      * nLabels].reshape(nHidden[-1], nLabels)\n",
    "\n",
    "    # Compute activations of the last hidden layer\n",
    "    activations = X\n",
    "    for h in range(len(nHidden)):\n",
    "        activations = np.tanh(activations @ (inputWeights if h == 0 else hiddenWeights[h-1]))\n",
    "\n",
    "    # Solve for the output weights using the normal equation\n",
    "    A = activations\n",
    "    A_T_A = A.T @ A\n",
    "    regularization_term = reg_lambda * np.eye(A_T_A.shape[0])\n",
    "    inverse_term = np.linalg.inv(A_T_A + regularization_term)\n",
    "    solution = inverse_term @ A.T @ y\n",
    "    w[offset:offset + nHidden[-1]\n",
    "      * nLabels] = solution.reshape(-1, 1)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration = 10000, training error = 0.165000, validation error = 0.174000\n",
      "Training iteration = 20000, training error = 0.177400, validation error = 0.202200\n",
      "Training iteration = 30000, training error = 0.157800, validation error = 0.167800\n",
      "Training iteration = 40000, training error = 0.143400, validation error = 0.163000\n",
      "Training iteration = 50000, training error = 0.146800, validation error = 0.170800\n",
      "Training iteration = 60000, training error = 0.164800, validation error = 0.190200\n",
      "Training iteration = 70000, training error = 0.136000, validation error = 0.146800\n",
      "Training iteration = 80000, training error = 0.138000, validation error = 0.156800\n",
      "Training iteration = 90000, training error = 0.129200, validation error = 0.143000\n",
      "Training iteration = 100000, training error = 0.151200, validation error = 0.174800\n",
      "Test error with final model = 0.149000\n",
      "Test error after fine-tuning = 0.151000\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('digits.mat')\n",
    "X = data['X']\n",
    "y = data['y'].flatten()\n",
    "yvalid = data['yvalid']\n",
    "ytest = data['ytest']\n",
    "n, d = X.shape  # 5000, 256\n",
    "nLabels = np.max(y)  # 10\n",
    "yExpanded = 2 * np.eye(nLabels)[y - 1] - 1  # turn into one-hot vector\n",
    "t = data['Xvalid'].shape[0]  # 5000\n",
    "t2 = data['Xtest'].shape[0]  # 1000\n",
    "X, mu, sigma = standardizeCols(X)\n",
    "X = np.hstack([np.ones((n, 1)), X])\n",
    "Xvalid, _, _ = standardizeCols(data['Xvalid'], mu, sigma)\n",
    "Xvalid = np.hstack([np.ones((t, 1)), Xvalid])\n",
    "Xtest, _, _ = standardizeCols(data['Xtest'], mu, sigma)\n",
    "Xtest = np.hstack([np.ones((t2, 1)), Xtest])\n",
    "\n",
    "nHidden = [10]\n",
    "d += 1\n",
    "\n",
    "nParams = d * nHidden[0]\n",
    "nParams += sum(nHidden[h-1] * nHidden[h] for h in range(1, len(nHidden)))\n",
    "nParams += nHidden[-1] * nLabels\n",
    "maxIter = 100000\n",
    "initialStepSize = 1e-3\n",
    "decayRate = 1e-5\n",
    "momentum = 0.9\n",
    "reg_lambda = 0.1\n",
    "\n",
    "w = np.random.randn(nParams, 1)\n",
    "w_diff = np.zeros_like(w)\n",
    "best_w = np.copy(w)\n",
    "best_validation_error = float('inf')\n",
    "\n",
    "\n",
    "for iter in range(0, maxIter):\n",
    "    stepSize = initialStepSize * (1 / (1 + decayRate * iter))\n",
    "    if (iter + 1) % (maxIter // 10) == 0:\n",
    "        yhat1 = MLPclassificationPredict(w, X, nHidden, nLabels)\n",
    "        train_error = np.mean(yhat1 != y.reshape(-1, 1))\n",
    "        yhat = MLPclassificationPredict(w, Xvalid, nHidden, nLabels)\n",
    "        validation_error = np.mean(yhat != yvalid)\n",
    "        print(f'Training iteration = {iter + 1}, training error = {train_error:.6f}, validation error = {validation_error:.6f}')\n",
    "\n",
    "        if validation_error < best_validation_error:\n",
    "            best_validation_error = validation_error\n",
    "            best_w = np.copy(w)\n",
    "\n",
    "    i = np.random.randint(n)\n",
    "    _, g = MLPclassificationLoss(\n",
    "        w, X[i:i+1], yExpanded[i:i+1], nHidden, nLabels, reg_lambda)\n",
    "    w_diff = momentum * w_diff - stepSize * g\n",
    "    w += w_diff\n",
    "\n",
    "# Evaluate test error\n",
    "yhat = MLPclassificationPredict(best_w, Xtest, nHidden, nLabels)\n",
    "test_error = np.mean(yhat != ytest)\n",
    "print(f'Test error with final model = {test_error:.6f}')\n",
    "\n",
    "# Finetuning the output weights\n",
    "fine_tuned_w(best_w, X, yExpanded, nHidden, nLabels, reg_lambda)\n",
    "yhat = MLPclassificationPredict(best_w, Xtest, nHidden, nLabels)\n",
    "test_error = np.mean(yhat != ytest)\n",
    "print(f'Test error after fine-tuning = {test_error:.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
