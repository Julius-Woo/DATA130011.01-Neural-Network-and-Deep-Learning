{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set: 16*16 grayscale 10 classes\n",
    "- digits.mat\n",
    "- X 5000*256\n",
    "- y 5000*1\n",
    "- Xvalid 5000*256\n",
    "- yvalid 5000*1\n",
    "- Xtest 1000*256\n",
    "- ytest 1000*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizeCols(M, mu=None, sigma2=None):\n",
    "    \"\"\"\n",
    "    Standardize each column of matrix M to have zero mean and unit standard deviation.\n",
    "    \n",
    "    Parameters:\n",
    "    M (numpy.ndarray): The input matrix.\n",
    "    mu (numpy.ndarray, optional): Precomputed mean of the columns.\n",
    "    sigma2 (numpy.ndarray, optional): Precomputed standard deviation of the columns.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The standardized matrix.\n",
    "    numpy.ndarray: Mean of the columns.\n",
    "    numpy.ndarray: Standard deviation of the columns.\n",
    "    \"\"\"\n",
    "    M = M.astype(float)  # Ensure M is float for precision\n",
    "    nrows, ncols = M.shape\n",
    "\n",
    "    if mu is None or sigma2 is None:\n",
    "        mu = np.mean(M, axis=0)\n",
    "        sigma2 = np.std(M, axis=0)\n",
    "        sigma2[sigma2 < np.finfo(float).eps] = 1  # Avoid division by zero\n",
    "\n",
    "    S = M - mu  # Subtract mean\n",
    "    if ncols > 0:\n",
    "        S = S / sigma2  # Divide by standard deviation\n",
    "\n",
    "    return S, mu, sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "def MLPclassificationPredict(w, X, nHidden, nLabels):\n",
    "    # w: 1-D, stores all weights\n",
    "    nInstances, nVars = X.shape\n",
    "\n",
    "    # Form Weights\n",
    "    offset = 0\n",
    "    inputWeights = w[offset:nVars*nHidden[0]].reshape(nVars, nHidden[0])\n",
    "    offset += nVars*nHidden[0]\n",
    "    hiddenWeights = []\n",
    "    for h in range(1, len(nHidden)):\n",
    "        size = nHidden[h-1] * nHidden[h]\n",
    "        hiddenWeights.append(\n",
    "            w[offset:offset+size].reshape(nHidden[h-1], nHidden[h]))\n",
    "        offset += size\n",
    "    outputWeights = w[offset:offset+nHidden[-1]\n",
    "                      * nLabels].reshape(nHidden[-1], nLabels)\n",
    "\n",
    "    # Compute Output\n",
    "    y = np.zeros((nInstances, nLabels))\n",
    "    for i in range(nInstances):\n",
    "        ip = [X[i] @ inputWeights]\n",
    "        fp = [np.tanh(ip[0])]\n",
    "        for h in range(1, len(nHidden)):\n",
    "            ip.append(fp[h-1] @ hiddenWeights[h-1])\n",
    "            fp.append(np.tanh(ip[h]))\n",
    "        y[i] = fp[-1] @ outputWeights\n",
    "\n",
    "    # Pick the class with the highest score\n",
    "    y = np.argmax(y, axis=1, keepdims=True) + 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sech2(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "# Loss and gradient\n",
    "def MLPclassificationLoss(w, X, y, nHidden, nLabels):\n",
    "    # X, y are all 2-D arrays\n",
    "    nInstances, nVars = X.shape\n",
    "\n",
    "    # Form Weights\n",
    "    offset = 0\n",
    "    inputWeights = w[offset:nVars*nHidden[0]].reshape(nVars, nHidden[0])\n",
    "    offset += nVars*nHidden[0]\n",
    "    hiddenWeights = []\n",
    "    for h in range(1, len(nHidden)):\n",
    "        size = nHidden[h-1] * nHidden[h]\n",
    "        hiddenWeights.append(\n",
    "            w[offset:offset+size].reshape(nHidden[h-1], nHidden[h]))\n",
    "        offset += size\n",
    "    outputWeights = w[offset:offset+nHidden[-1]\n",
    "                      * nLabels].reshape(nHidden[-1], nLabels)\n",
    "\n",
    "    f = 0  # Loss\n",
    "    gInput = np.zeros_like(inputWeights)  # Gradient for input weights\n",
    "    gHidden = [np.zeros_like(hw) for hw in hiddenWeights]  # Gradient for hidden weights\n",
    "    gOutput = np.zeros_like(outputWeights)  # Gradient for output weights\n",
    "\n",
    "\n",
    "    # Compute Output\n",
    "    for i in range(nInstances):\n",
    "        ip = [None] * len(nHidden)\n",
    "        fp = [None] * len(nHidden)\n",
    "\n",
    "        ip[0] = X[i, :] @ inputWeights  # 1-D array\n",
    "        fp[0] = np.tanh(ip[0])  # 1-D array\n",
    "        for h in range(1, len(nHidden)):\n",
    "            ip[h] = fp[h-1] @ hiddenWeights[h-1]\n",
    "            fp[h] = np.tanh(ip[h])\n",
    "        yhat = fp[-1] @ outputWeights\n",
    "\n",
    "        relativeErr = yhat - y[i, :]\n",
    "        f += np.sum(relativeErr**2)\n",
    "\n",
    "        err = 2 * relativeErr\n",
    "\n",
    "        # Output Weights Gradient\n",
    "        gOutput += np.outer(fp[-1], err)\n",
    "\n",
    "        if len(nHidden) > 1:\n",
    "            backprop = np.zeros_like(fp[-1])\n",
    "            for c in range(nLabels):\n",
    "                backprop[c] = err[c] * sech2(ip[-1]) * outputWeights[:, c].T\n",
    "                gHidden[-1] += np.outer(fp[-2], backprop[c])\n",
    "            backprop = np.sum(backprop, axis=0)  # Sum over classes\n",
    "\n",
    "            # Other Hidden Layer Gradients\n",
    "            for h in range(len(nHidden)-2, -1, -1):\n",
    "                backprop = (backprop @ hiddenWeights[h].T) * sech2(ip[h])\n",
    "                if h > 0:\n",
    "                    gHidden[h-1] += np.outer(fp[h-1], backprop)\n",
    "                else:\n",
    "                    # Input weights\n",
    "                    gInput += np.outer(X[i, :], backprop)\n",
    "        else:\n",
    "            # Input Weights Gradient for single hidden layer case\n",
    "            for c in range(nLabels):\n",
    "                gInput += err[c] * np.outer(X[i], sech2(ip[-1]) * outputWeights[:, c].T)\n",
    "\n",
    "    # Put Gradient into vector\n",
    "    g = np.zeros_like(w)\n",
    "    g[:nVars * nHidden[0]] = gInput.reshape(-1, 1)\n",
    "    offset = nVars * nHidden[0]\n",
    "    for h in range(len(nHidden)-1):\n",
    "        g[offset : offset + nHidden[h] * nHidden[h+1]] = gHidden[h].reshape(-1, 1)\n",
    "        offset += nHidden[h] * nHidden[h+1]\n",
    "    g[offset : offset + nHidden[-1] * nLabels] = gOutput.reshape(-1, 1)\n",
    "\n",
    "    return f, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration = 5000,             validation error = 0.611800\n",
      "Training iteration = 10000,             validation error = 0.608400\n",
      "Training iteration = 15000,             validation error = 0.577800\n",
      "Training iteration = 20000,             validation error = 0.569000\n",
      "Training iteration = 25000,             validation error = 0.557800\n",
      "Training iteration = 30000,             validation error = 0.520000\n",
      "Training iteration = 35000,             validation error = 0.529600\n",
      "Training iteration = 40000,             validation error = 0.496800\n",
      "Training iteration = 45000,             validation error = 0.490400\n",
      "Training iteration = 50000,             validation error = 0.523800\n",
      "Training iteration = 55000,             validation error = 0.497600\n",
      "Training iteration = 60000,             validation error = 0.476000\n",
      "Training iteration = 65000,             validation error = 0.488200\n",
      "Training iteration = 70000,             validation error = 0.491000\n",
      "Training iteration = 75000,             validation error = 0.476400\n",
      "Training iteration = 80000,             validation error = 0.471600\n",
      "Training iteration = 85000,             validation error = 0.494400\n",
      "Training iteration = 90000,             validation error = 0.474600\n",
      "Training iteration = 95000,             validation error = 0.465200\n",
      "Training iteration = 100000,             validation error = 0.456400\n",
      "Test error with final model = 0.470000\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('digits.mat')\n",
    "X = data['X']\n",
    "y = data['y'].flatten()\n",
    "yvalid = data['yvalid']\n",
    "ytest = data['ytest']\n",
    "\n",
    "n, d = X.shape  # 5000, 256\n",
    "nLabels = np.max(y)  # 10\n",
    "yExpanded = 2 * np.eye(nLabels)[y - 1] - 1  # turn into one-hot vector\n",
    "t = data['Xvalid'].shape[0]  # 5000\n",
    "t2 = data['Xtest'].shape[0]  # 1000\n",
    "\n",
    "\n",
    "# Standardize columns and add bias\n",
    "X, mu, sigma = standardizeCols(X)\n",
    "X = np.hstack([np.ones((n, 1)), X])\n",
    "\n",
    "# Apply the same transformation to the validation/test data\n",
    "Xvalid, _, _ = standardizeCols(data['Xvalid'], mu, sigma)\n",
    "Xvalid = np.hstack([np.ones((t, 1)), Xvalid])\n",
    "\n",
    "Xtest, _, _ = standardizeCols(data['Xtest'], mu, sigma)\n",
    "Xtest = np.hstack([np.ones((t2, 1)), Xtest])\n",
    "\n",
    "\n",
    "# Choose network structure\n",
    "nHidden = [10]\n",
    "d += 1  # Adjust (n, d) = X.shape\n",
    "\n",
    "# Count number of parameters\n",
    "nParams = d * nHidden[0]  # Input layer and first hidden layer\n",
    "nParams += sum(nHidden[h-1] * nHidden[h] for h in range(1, len(nHidden)))\n",
    "nParams += nHidden[-1] * nLabels  # Last hidden layer and output layer\n",
    "# initialize weights\n",
    "w = np.random.randn(nParams, 1)\n",
    "\n",
    "# Train with stochastic gradient\n",
    "maxIter = 100000\n",
    "stepSize = 1e-3\n",
    "\n",
    "for iter in range(0, maxIter):\n",
    "    if (iter + 1) % (maxIter // 20) == 0:\n",
    "        yhat = MLPclassificationPredict(w, Xvalid, nHidden, nLabels)\n",
    "        validation_error = np.mean(yhat != yvalid)\n",
    "        print(f'Training iteration = {iter + 1}, \\\n",
    "            validation error = {validation_error:.6f}')\n",
    "\n",
    "    # batch = 1\n",
    "    i = np.random.randint(n)\n",
    "    f, g = MLPclassificationLoss(w, X[i:i+1], yExpanded[i:i+1], nHidden, nLabels)\n",
    "    w -= stepSize * g\n",
    "\n",
    "# Evaluate test error\n",
    "yhat = MLPclassificationPredict(w, Xtest, nHidden, nLabels)\n",
    "test_error = np.mean(yhat != ytest)\n",
    "print(f'Test error with final model = {test_error:.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
