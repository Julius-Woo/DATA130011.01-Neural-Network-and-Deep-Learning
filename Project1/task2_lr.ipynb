{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task2. Change the training procedure by modifying the sequence of step-sizes or using different step-sizes for different\n",
    "variables (using momentum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "def standardizeCols(M, mu=None, sigma2=None):\n",
    "    M = M.astype(float)  # Ensure M is float for precision\n",
    "    nrows, ncols = M.shape\n",
    "    if mu is None or sigma2 is None:\n",
    "        mu = np.mean(M, axis=0)\n",
    "        sigma2 = np.std(M, axis=0)\n",
    "        sigma2[sigma2 < np.finfo(float).eps] = 1  # Avoid division by zero\n",
    "    S = M - mu  # Subtract mean\n",
    "    if ncols > 0:\n",
    "        S = S / sigma2  # Divide by standard deviation\n",
    "    return S, mu, sigma2\n",
    "\n",
    "\n",
    "def form_weights(w, nVars, nHidden, nLabels):\n",
    "    offset = 0\n",
    "    inputWeights = w[offset:nVars * nHidden[0]].reshape(nVars, nHidden[0])\n",
    "    offset += nVars * nHidden[0]\n",
    "    hiddenWeights = []\n",
    "    for h in range(1, len(nHidden)):\n",
    "        size = nHidden[h-1] * nHidden[h]\n",
    "        hiddenWeights.append(\n",
    "            w[offset:offset+size].reshape(nHidden[h-1], nHidden[h]))\n",
    "        offset += size\n",
    "    outputWeights = w[offset:offset + nHidden[-1]\n",
    "                      * nLabels].reshape(nHidden[-1], nLabels)\n",
    "    return inputWeights, hiddenWeights, outputWeights\n",
    "\n",
    "\n",
    "def sech2(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "def MLPclassificationPredict(w, X, nHidden, nLabels):\n",
    "    # w: 1-D, stores all weights\n",
    "    nInstances, nVars = X.shape\n",
    "\n",
    "    # Form Weights\n",
    "    inputWeights, hiddenWeights, outputWeights = form_weights(w, nVars, nHidden, nLabels)\n",
    "\n",
    "    # Compute Output in batch\n",
    "    activations = X\n",
    "    for h in range(len(nHidden)):\n",
    "        activations = np.tanh(activations @ (inputWeights if h == 0 else hiddenWeights[h-1]))\n",
    "    y = activations @ outputWeights\n",
    "\n",
    "    # Pick the class with the highest score\n",
    "    y = np.argmax(y, axis=1, keepdims=True) + 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPclassificationLoss(w, X, y, nHidden, nLabels):\n",
    "    # y should be one-hot encoded\n",
    "    nInstances, nVars = X.shape\n",
    "    inputWeights, hiddenWeights, outputWeights = form_weights(w, nVars, nHidden, nLabels)\n",
    "\n",
    "    # Forward pass\n",
    "    activations = [X]\n",
    "    for h in range(len(nHidden)):\n",
    "        z = activations[-1] @ (inputWeights if h == 0 else hiddenWeights[h-1])\n",
    "        a = np.tanh(z)\n",
    "        activations.append(a)\n",
    "\n",
    "    # Output layer\n",
    "    yhat = activations[-1] @ outputWeights\n",
    "    f = np.sum((yhat - y)**2)  # Loss\n",
    "\n",
    "    # Backpropagation\n",
    "    gOutput = 2 * activations[-1].T @ (yhat - y)\n",
    "\n",
    "    # Gradients for hidden and input weights\n",
    "    gHidden = []\n",
    "    delta = 2 * (yhat - y) @ outputWeights.T * sech2(activations[-1])\n",
    "    for h in range(len(nHidden) - 1, 0, -1):\n",
    "        gHidden.append(activations[h].T @ delta)\n",
    "        delta = (delta @ hiddenWeights[h-1].T) * sech2(activations[h])\n",
    "    gHidden.append(activations[0].T @ delta)  # Input weights gradient\n",
    "    gHidden.reverse()\n",
    "\n",
    "    # Flatten gradients into vector\n",
    "    gradients = [gHidden[0].flatten()]\n",
    "    for g in gHidden[1:]:\n",
    "        gradients.append(g.flatten())\n",
    "    gradients.append(gOutput.flatten())\n",
    "    g = np.concatenate(gradients)\n",
    "\n",
    "    return f, g.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the sequence of step-sizes\n",
    "\n",
    "We implement learning rate decay: current_lr = initial_lr / (1 + decayRate * iteration), and test with different initial learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Training with initialStepSize = 0.1-----\n",
      "Training iteration = 20000,                 validation error = 0.879600\n",
      "Training iteration = 40000,                 validation error = 0.895400\n",
      "Training iteration = 60000,                 validation error = 0.881800\n",
      "Training iteration = 80000,                 validation error = 0.873200\n",
      "Training iteration = 100000,                 validation error = 0.901200\n",
      "Test error with final model = 0.900000\n",
      "\n",
      "-----Training with initialStepSize = 0.01-----\n",
      "Training iteration = 20000,                 validation error = 0.212200\n",
      "Training iteration = 40000,                 validation error = 0.210200\n",
      "Training iteration = 60000,                 validation error = 0.228600\n",
      "Training iteration = 80000,                 validation error = 0.220600\n",
      "Training iteration = 100000,                 validation error = 0.214600\n",
      "Test error with final model = 0.230000\n",
      "\n",
      "-----Training with initialStepSize = 0.001-----\n",
      "Training iteration = 20000,                 validation error = 0.354200\n",
      "Training iteration = 40000,                 validation error = 0.279600\n",
      "Training iteration = 60000,                 validation error = 0.231800\n",
      "Training iteration = 80000,                 validation error = 0.217000\n",
      "Training iteration = 100000,                 validation error = 0.202800\n",
      "Test error with final model = 0.220000\n",
      "\n",
      "-----Training with initialStepSize = 0.0001-----\n",
      "Training iteration = 20000,                 validation error = 0.695200\n",
      "Training iteration = 40000,                 validation error = 0.600400\n",
      "Training iteration = 60000,                 validation error = 0.556000\n",
      "Training iteration = 80000,                 validation error = 0.532000\n",
      "Training iteration = 100000,                 validation error = 0.515600\n",
      "Test error with final model = 0.503000\n",
      "\n",
      "-----Training with initialStepSize = 1e-05-----\n",
      "Training iteration = 20000,                 validation error = 0.858200\n",
      "Training iteration = 40000,                 validation error = 0.845400\n",
      "Training iteration = 60000,                 validation error = 0.835200\n",
      "Training iteration = 80000,                 validation error = 0.826800\n",
      "Training iteration = 100000,                 validation error = 0.821000\n",
      "Test error with final model = 0.835000\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('digits.mat')\n",
    "X = data['X']\n",
    "y = data['y'].flatten()\n",
    "yvalid = data['yvalid']\n",
    "ytest = data['ytest']\n",
    "\n",
    "n, d = X.shape  # 5000, 256\n",
    "nLabels = np.max(y)  # 10\n",
    "yExpanded = 2 * np.eye(nLabels)[y - 1] - 1  # turn into one-hot vector\n",
    "t = data['Xvalid'].shape[0]  # 5000\n",
    "t2 = data['Xtest'].shape[0]  # 1000\n",
    "\n",
    "\n",
    "# Standardize columns and add bias\n",
    "X, mu, sigma = standardizeCols(X)\n",
    "X = np.hstack([np.ones((n, 1)), X])\n",
    "\n",
    "# Apply the same transformation to the validation/test data\n",
    "Xvalid, _, _ = standardizeCols(data['Xvalid'], mu, sigma)\n",
    "Xvalid = np.hstack([np.ones((t, 1)), Xvalid])\n",
    "\n",
    "Xtest, _, _ = standardizeCols(data['Xtest'], mu, sigma)\n",
    "Xtest = np.hstack([np.ones((t2, 1)), Xtest])\n",
    "\n",
    "\n",
    "nHidden = [10]  # Single hidden layer with 10 neurons\n",
    "d += 1  # Adjust (n, d) = X.shape\n",
    "\n",
    "# Count number of parameters\n",
    "nParams = d * nHidden[0]  # Input layer and first hidden layer\n",
    "nParams += sum(nHidden[h-1] * nHidden[h] for h in range(1, len(nHidden)))\n",
    "nParams += nHidden[-1] * nLabels  # Last hidden layer and output layer\n",
    "\n",
    "\n",
    "# Train with stochastic gradient\n",
    "maxIter = 100000\n",
    "initialStepSizes = [0.1, 0.01, 1e-3, 1e-4, 1e-5]\n",
    "decayRate = 1e-5\n",
    "\n",
    "for initialStepSize in initialStepSizes:\n",
    "    print(f'\\n-----Training with initialStepSize = {initialStepSize}-----')\n",
    "    w = np.random.randn(nParams, 1)\n",
    "    for iter in range(0, maxIter):\n",
    "        stepSize = initialStepSize * (1 / (1 + decayRate * iter))\n",
    "        if (iter + 1) % (maxIter // 5) == 0:\n",
    "            yhat = MLPclassificationPredict(w, Xvalid, nHidden, nLabels)\n",
    "            validation_error = np.mean(yhat != yvalid)\n",
    "            print(f'Training iteration = {iter + 1}, \\\n",
    "                validation error = {validation_error:.6f}')\n",
    "\n",
    "        # batch = 1\n",
    "        i = np.random.randint(n)\n",
    "        f, g = MLPclassificationLoss(w, X[i:i+1], yExpanded[i:i+1], nHidden, nLabels)\n",
    "        w -= stepSize * g\n",
    "\n",
    "    # Evaluate test error\n",
    "    yhat = MLPclassificationPredict(w, Xtest, nHidden, nLabels)\n",
    "    test_error = np.mean(yhat != ytest)\n",
    "    print(f'Test error with final model = {test_error:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial learning rate of 0.01 or 0.001 is good for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Training with momentum = 0.5-----\n",
      "Training iteration = 20000,                 validation error = 0.295000\n",
      "Training iteration = 40000,                 validation error = 0.240200\n",
      "Training iteration = 60000,                 validation error = 0.228600\n",
      "Training iteration = 80000,                 validation error = 0.212200\n",
      "Training iteration = 100000,                 validation error = 0.213800\n",
      "Test error with final model = 0.214000\n",
      "\n",
      "-----Training with momentum = 0.6-----\n",
      "Training iteration = 20000,                 validation error = 0.306600\n",
      "Training iteration = 40000,                 validation error = 0.244000\n",
      "Training iteration = 60000,                 validation error = 0.216000\n",
      "Training iteration = 80000,                 validation error = 0.214400\n",
      "Training iteration = 100000,                 validation error = 0.202400\n",
      "Test error with final model = 0.211000\n",
      "\n",
      "-----Training with momentum = 0.7-----\n",
      "Training iteration = 20000,                 validation error = 0.254400\n",
      "Training iteration = 40000,                 validation error = 0.227200\n",
      "Training iteration = 60000,                 validation error = 0.235400\n",
      "Training iteration = 80000,                 validation error = 0.225400\n",
      "Training iteration = 100000,                 validation error = 0.209400\n",
      "Test error with final model = 0.203000\n",
      "\n",
      "-----Training with momentum = 0.8-----\n",
      "Training iteration = 20000,                 validation error = 0.262800\n",
      "Training iteration = 40000,                 validation error = 0.248800\n",
      "Training iteration = 60000,                 validation error = 0.252400\n",
      "Training iteration = 80000,                 validation error = 0.245800\n",
      "Training iteration = 100000,                 validation error = 0.233600\n",
      "Test error with final model = 0.232000\n",
      "\n",
      "-----Training with momentum = 0.9-----\n",
      "Training iteration = 20000,                 validation error = 0.242400\n",
      "Training iteration = 40000,                 validation error = 0.231200\n",
      "Training iteration = 60000,                 validation error = 0.208600\n",
      "Training iteration = 80000,                 validation error = 0.230600\n",
      "Training iteration = 100000,                 validation error = 0.192000\n",
      "Test error with final model = 0.184000\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('digits.mat')\n",
    "X = data['X']\n",
    "y = data['y'].flatten()\n",
    "yvalid = data['yvalid']\n",
    "ytest = data['ytest']\n",
    "\n",
    "n, d = X.shape  # 5000, 256\n",
    "nLabels = np.max(y)  # 10\n",
    "yExpanded = 2 * np.eye(nLabels)[y - 1] - 1  # turn into one-hot vector\n",
    "t = data['Xvalid'].shape[0]  # 5000\n",
    "t2 = data['Xtest'].shape[0]  # 1000\n",
    "\n",
    "\n",
    "# Standardize columns and add bias\n",
    "X, mu, sigma = standardizeCols(X)\n",
    "X = np.hstack([np.ones((n, 1)), X])\n",
    "\n",
    "# Apply the same transformation to the validation/test data\n",
    "Xvalid, _, _ = standardizeCols(data['Xvalid'], mu, sigma)\n",
    "Xvalid = np.hstack([np.ones((t, 1)), Xvalid])\n",
    "\n",
    "Xtest, _, _ = standardizeCols(data['Xtest'], mu, sigma)\n",
    "Xtest = np.hstack([np.ones((t2, 1)), Xtest])\n",
    "\n",
    "\n",
    "nHidden = [10]  # Single hidden layer with 10 neurons\n",
    "d += 1  # Adjust (n, d) = X.shape\n",
    "\n",
    "nParams = d * nHidden[0]\n",
    "nParams += sum(nHidden[h-1] * nHidden[h] for h in range(1, len(nHidden)))\n",
    "nParams += nHidden[-1] * nLabels\n",
    "maxIter = 100000\n",
    "initialStepSize = 1e-3\n",
    "stepSize = initialStepSize\n",
    "decayRate = 1e-5\n",
    "momentums = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for momentum in momentums:\n",
    "    print(f'\\n-----Training with momentum = {momentum}-----')\n",
    "    w = np.random.randn(nParams, 1)\n",
    "    w_diff = np.zeros_like(w)\n",
    "    for iter in range(0, maxIter):\n",
    "        stepSize = initialStepSize * (1 / (1 + decayRate * iter))\n",
    "        if (iter + 1) % (maxIter // 5) == 0:\n",
    "            yhat = MLPclassificationPredict(w, Xvalid, nHidden, nLabels)\n",
    "            validation_error = np.mean(yhat != yvalid)\n",
    "            print(f'Training iteration = {iter + 1}, \\\n",
    "                validation error = {validation_error:.6f}')\n",
    "        i = np.random.randint(n)\n",
    "        f, g = MLPclassificationLoss(\n",
    "            w, X[i:i+1], yExpanded[i:i+1], nHidden, nLabels)\n",
    "        w_diff = momentum * w_diff - stepSize * g\n",
    "        w += w_diff\n",
    "\n",
    "    # Evaluate test error\n",
    "    yhat = MLPclassificationPredict(w, Xtest, nHidden, nLabels)\n",
    "    test_error = np.mean(yhat != ytest)\n",
    "    print(f'Test error with final model = {test_error:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A momentum strength of 0.9 is good for this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
