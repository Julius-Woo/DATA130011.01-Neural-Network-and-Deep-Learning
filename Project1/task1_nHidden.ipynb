{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task1. Change the network structure: the vector *nHidden* specifies the number of hidden units in each layer.\n",
    "\n",
    "\n",
    "Some points to note:\n",
    "1. We use as many matrix operations as possible to make the code more efficient, such as making forward passes for all the training data at once (eliminating the need for a for loop of instances).\n",
    "2. We encapsulate the weight forming process in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "def standardizeCols(M, mu=None, sigma2=None):\n",
    "    \"\"\"\n",
    "    Standardize each column of matrix M to have zero mean and unit standard deviation.\n",
    "    \n",
    "    Parameters:\n",
    "    M (numpy.ndarray): The input matrix.\n",
    "    mu (numpy.ndarray, optional): Precomputed mean of the columns.\n",
    "    sigma2 (numpy.ndarray, optional): Precomputed standard deviation of the columns.\n",
    "    \n",
    "    Returns:\n",
    "    S (numpy.ndarray): The standardized matrix.\n",
    "    mu (numpy.ndarray): Mean of the columns.\n",
    "    sigma2 (numpy.ndarray): Standard deviation of the columns.\n",
    "    \"\"\"\n",
    "    M = M.astype(float)  # Ensure M is float for precision\n",
    "    nrows, ncols = M.shape\n",
    "\n",
    "    if mu is None or sigma2 is None:\n",
    "        mu = np.mean(M, axis=0)\n",
    "        sigma2 = np.std(M, axis=0)\n",
    "        sigma2[sigma2 < np.finfo(float).eps] = 1  # Avoid division by zero\n",
    "\n",
    "    S = M - mu  # Subtract mean\n",
    "    if ncols > 0:\n",
    "        S = S / sigma2  # Divide by standard deviation\n",
    "\n",
    "    return S, mu, sigma2\n",
    "\n",
    "\n",
    "def form_weights(w, nVars, nHidden, nLabels):\n",
    "    offset = 0\n",
    "    inputWeights = w[offset:nVars * nHidden[0]].reshape(nVars, nHidden[0])\n",
    "    offset += nVars * nHidden[0]\n",
    "    hiddenWeights = []\n",
    "    for h in range(1, len(nHidden)):\n",
    "        size = nHidden[h-1] * nHidden[h]\n",
    "        hiddenWeights.append(\n",
    "            w[offset:offset+size].reshape(nHidden[h-1], nHidden[h]))\n",
    "        offset += size\n",
    "    outputWeights = w[offset:offset + nHidden[-1]\n",
    "                      * nLabels].reshape(nHidden[-1], nLabels)\n",
    "    return inputWeights, hiddenWeights, outputWeights\n",
    "\n",
    "\n",
    "def sech2(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "def MLPclassificationPredict(w, X, nHidden, nLabels):\n",
    "    # w: 1-D, stores all weights\n",
    "    nInstances, nVars = X.shape\n",
    "\n",
    "    # Form Weights\n",
    "    inputWeights, hiddenWeights, outputWeights = form_weights(w, nVars, nHidden, nLabels)\n",
    "\n",
    "    # Compute Output in batch\n",
    "    activations = X\n",
    "    for h in range(len(nHidden)):\n",
    "        activations = np.tanh(activations @ (inputWeights if h == 0 else hiddenWeights[h-1]))\n",
    "    y = activations @ outputWeights\n",
    "\n",
    "    # Pick the class with the highest score\n",
    "    y = np.argmax(y, axis=1, keepdims=True) + 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPclassificationLoss(w, X, y, nHidden, nLabels):\n",
    "    # y should be one-hot encoded\n",
    "    nInstances, nVars = X.shape\n",
    "    inputWeights, hiddenWeights, outputWeights = form_weights(w, nVars, nHidden, nLabels)\n",
    "\n",
    "    # Forward pass\n",
    "    activations = [X]\n",
    "    for h in range(len(nHidden)):\n",
    "        z = activations[-1] @ (inputWeights if h == 0 else hiddenWeights[h-1])\n",
    "        a = np.tanh(z)\n",
    "        activations.append(a)\n",
    "\n",
    "    # Output layer\n",
    "    yhat = activations[-1] @ outputWeights\n",
    "    f = np.sum((yhat - y)**2)  # Loss\n",
    "\n",
    "    # Backpropagation\n",
    "    gOutput = 2 * activations[-1].T @ (yhat - y)\n",
    "\n",
    "    # Gradients for hidden and input weights\n",
    "    gHidden = []\n",
    "    delta = 2 * (yhat - y) @ outputWeights.T * sech2(activations[-1])\n",
    "    for h in range(len(nHidden) - 1, 0, -1):\n",
    "        gHidden.append(activations[h].T @ delta)\n",
    "        delta = (delta @ hiddenWeights[h-1].T) * sech2(activations[h])\n",
    "    gHidden.append(activations[0].T @ delta)  # Input weights gradient\n",
    "    gHidden.reverse()\n",
    "\n",
    "    # Flatten gradients into vector\n",
    "    gradients = [gHidden[0].flatten()]\n",
    "    for g in gHidden[1:]:\n",
    "        gradients.append(g.flatten())\n",
    "    gradients.append(gOutput.flatten())\n",
    "    g = np.concatenate(gradients)\n",
    "\n",
    "    return f, g.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Training with nHiddens = [10]-----\n",
      "Training iteration = 20000,                 validation error = 0.420000\n",
      "Training iteration = 40000,                 validation error = 0.308400\n",
      "Training iteration = 60000,                 validation error = 0.250800\n",
      "Training iteration = 80000,                 validation error = 0.229400\n",
      "Training iteration = 100000,                 validation error = 0.225000\n",
      "Test error with final model = 0.213000\n",
      "\n",
      "-----Training with nHiddens = [128]-----\n",
      "Training iteration = 20000,                 validation error = 0.224400\n",
      "Training iteration = 40000,                 validation error = 0.205400\n",
      "Training iteration = 60000,                 validation error = 0.195400\n",
      "Training iteration = 80000,                 validation error = 0.210600\n",
      "Training iteration = 100000,                 validation error = 0.174000\n",
      "Test error with final model = 0.167000\n",
      "\n",
      "-----Training with nHiddens = [10, 10]-----\n",
      "Training iteration = 20000,                 validation error = 0.469200\n",
      "Training iteration = 40000,                 validation error = 0.337200\n",
      "Training iteration = 60000,                 validation error = 0.303200\n",
      "Training iteration = 80000,                 validation error = 0.269400\n",
      "Training iteration = 100000,                 validation error = 0.227400\n",
      "Test error with final model = 0.219000\n",
      "\n",
      "-----Training with nHiddens = [128, 128]-----\n",
      "Training iteration = 20000,                 validation error = 0.243000\n",
      "Training iteration = 40000,                 validation error = 0.194600\n",
      "Training iteration = 60000,                 validation error = 0.176200\n",
      "Training iteration = 80000,                 validation error = 0.159800\n",
      "Training iteration = 100000,                 validation error = 0.154000\n",
      "Test error with final model = 0.165000\n",
      "\n",
      "-----Training with nHiddens = [10, 10, 10]-----\n",
      "Training iteration = 20000,                 validation error = 0.419400\n",
      "Training iteration = 40000,                 validation error = 0.315000\n",
      "Training iteration = 60000,                 validation error = 0.276800\n",
      "Training iteration = 80000,                 validation error = 0.274800\n",
      "Training iteration = 100000,                 validation error = 0.291200\n",
      "Test error with final model = 0.296000\n",
      "\n",
      "-----Training with nHiddens = [128, 128, 128]-----\n",
      "Training iteration = 20000,                 validation error = 0.286800\n",
      "Training iteration = 40000,                 validation error = 0.195200\n",
      "Training iteration = 60000,                 validation error = 0.165400\n",
      "Training iteration = 80000,                 validation error = 0.147200\n",
      "Training iteration = 100000,                 validation error = 0.142600\n",
      "Test error with final model = 0.133000\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('digits.mat')\n",
    "X = data['X']\n",
    "y = data['y'].flatten()\n",
    "yvalid = data['yvalid']\n",
    "ytest = data['ytest']\n",
    "\n",
    "n, d = X.shape  # 5000, 256\n",
    "nLabels = np.max(y)  # 10\n",
    "yExpanded = 2 * np.eye(nLabels)[y - 1] - 1  # turn into one-hot vector\n",
    "t = data['Xvalid'].shape[0]  # 5000\n",
    "t2 = data['Xtest'].shape[0]  # 1000\n",
    "\n",
    "\n",
    "# Standardize columns and add bias\n",
    "X, mu, sigma = standardizeCols(X)\n",
    "X = np.hstack([np.ones((n, 1)), X])\n",
    "\n",
    "# Apply the same transformation to the validation/test data\n",
    "Xvalid, _, _ = standardizeCols(data['Xvalid'], mu, sigma)\n",
    "Xvalid = np.hstack([np.ones((t, 1)), Xvalid])\n",
    "\n",
    "Xtest, _, _ = standardizeCols(data['Xtest'], mu, sigma)\n",
    "Xtest = np.hstack([np.ones((t2, 1)), Xtest])\n",
    "\n",
    "\n",
    "# Choose network structure\n",
    "nHiddens = [[10], [128], [10, 10], [128, 128], [10, 10, 10], [128, 128, 128]]\n",
    "d += 1  # Adjust (n, d) = X.shape\n",
    "\n",
    "for nHidden in nHiddens:\n",
    "    print(f'\\n-----Training with nHiddens = {nHidden}-----')\n",
    "    nParams = d * nHidden[0]  # Input layer and first hidden layer\n",
    "    nParams += sum(nHidden[h-1] * nHidden[h] for h in range(1, len(nHidden)))\n",
    "    nParams += nHidden[-1] * nLabels  # Last hidden layer and output layer\n",
    "    # initialize weights\n",
    "    w = np.random.randn(nParams, 1)\n",
    "\n",
    "    # Train with stochastic gradient\n",
    "    maxIter = 100000\n",
    "    stepSize = 1e-3\n",
    "\n",
    "    for iter in range(0, maxIter):\n",
    "        if (iter + 1) % (maxIter // 5) == 0:\n",
    "            yhat = MLPclassificationPredict(w, Xvalid, nHidden, nLabels)\n",
    "            validation_error = np.mean(yhat != yvalid)\n",
    "            print(f'Training iteration = {iter + 1}, \\\n",
    "                validation error = {validation_error:.6f}')\n",
    "\n",
    "        # batch = 1\n",
    "        i = np.random.randint(n)\n",
    "        f, g = MLPclassificationLoss(w, X[i:i+1], yExpanded[i:i+1], nHidden, nLabels)\n",
    "        w -= stepSize * g\n",
    "\n",
    "    # Evaluate test error\n",
    "    yhat = MLPclassificationPredict(w, Xtest, nHidden, nLabels)\n",
    "    test_error = np.mean(yhat != ytest)\n",
    "    print(f'Test error with final model = {test_error:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that validation error decreases as the number of hidden units increases or the number of layers increases. The increase of unit numbers affects the result more significantly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
